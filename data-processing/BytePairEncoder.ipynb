{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d12f9c91-b291-4892-a844-515db1802de1",
   "metadata": {},
   "source": [
    "# BYTE PAIR ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9bf026-f494-4b46-8434-ad48ce53589f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We implemented a simple tokenization scheme. We took sentences, converted them into tokens and then we converted this into\n",
    "vocabulary so then every word of the token was arranged in an ascending order and then a ID or a numerical ID or a token ID was \n",
    "assigned to each token so here every word was a unique token along with special characters like comma full stop exclamation mark\n",
    "etc.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fa374e-9494-44c8-8918-725e26a8d240",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The tokenization algorithms are of three types <br>\n",
    "* word based tokenizer <br>\n",
    "* subword based tokenizer <br>\n",
    "* character based tokenizer <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdf4b94-dc9e-4764-b2ec-1859d4ff3f0c",
   "metadata": {},
   "source": [
    "## Word Based tokenizer\n",
    "Consider the example \"the fox chased the dog,\" where the tokens are \"the,\" \"fox,\" \"chased,\" \"the,\" and \"dog.\" Each word serves as an individual token, illustrating a word-based tokenizer. In this approach, tokens are defined at word boundaries.\n",
    "\n",
    "However, there are challenges associated with word-based tokenizers. The primary issue arises when dealing with words that are not included in the vocabulary. Typically, you have a large dataset that you break down into sentences, and then into individual words, assigning a token ID to each. But when a user interacts with a language model and inputs a word not found in the vocabulary-referred to as \"out of vocabulary\" (OOV) words-the tokenization process often encounters errors. Handling OOV words is notoriously difficult with word-based tokenization.\n",
    "\n",
    "A vocabulary is essentially a dictionary of tokens organized in ascending order, where each token corresponds to a unique token ID. Another issue with this tokenization method is exemplified by the words \"boy\" and \"boys.\" Although they are similar, they are treated as separate tokens, with potentially very different token IDs, depending on their position in the vocabulary. This approach fails to capture the semantic similarity between such words, as \"boy\" is the root word for \"boys,\" yet both are treated distinctly.\n",
    "\n",
    "Additionally, attempting to account for all possible word combinations is not scalable, as it significantly enlarges the vocabulary size. These are the main challenges associated with word-based tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4325c481-5156-472e-964a-a6419f26cb2a",
   "metadata": {},
   "source": [
    "## Character based tokenizer\n",
    "On the opposite end of the spectrum is character-based tokenization, another widely used tokenization scheme. Instead of using whole words as tokens, this approach treats individual characters as tokens. For instance, in the sentence \"my hobby is playing Cricket,\" each character becomes a token. Ignoring spaces, the first few tokens would be 'm', 'y', 'h', 'o', 'b', and so on.\n",
    "\n",
    "This method results in a very small vocabulary size, as every language has a limited number of characters. In English, there are only 256 characters, compared to around 170,000 to 200,000 words. One of the significant advantages of character-based tokenization is the elimination of the \"out of vocabulary\" (OOV) problem. Any new sentence can be broken down into characters, all of which will belong to the existing set of 256 characters in the vocabulary, thus avoiding OOV issues. This also means that the vocabulary size is manageable, unlike word-based tokenization, which requires a massive vocabulary to cover all possible words.\n",
    "\n",
    "While character-based tokenization offers solutions like OOV problem elimination and computational efficiency due to its small vocabulary size, it also has drawbacks. The primary issue is the loss of semantic meaning associated with words. Language models rely on the meaning of words, and by breaking sentences into individual characters, this meaning is lost. For example, related words like \"boy\" and \"boys\" share a common meaning, but this connection is lost in character-level tokenization.\n",
    "\n",
    "Another downside is that the tokenized sequence becomes much longer than the original text. For example, the word \"dinosaur\" would be treated as one token in word-based tokenization but split into eight separate tokens ('d', 'i', 'n', 'o', 's', 'a', 'u', 'r') in character-based tokenization. This results in a longer tokenized sequence, which can be cumbersome to handle.\n",
    "\n",
    "In summary, while word-based tokenization struggles with large vocabulary sizes and OOV words, it preserves semantic meaning. Character-based tokenization resolves the OOV issue and maintains a smaller vocabulary but sacrifices word meaning and increases tokenized sequence length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fecfd2-2f3f-4d94-a453-3eff9562a79c",
   "metadata": {},
   "source": [
    "## Subword Tokenizer\n",
    "\n",
    "Let's explore subword tokenization, a method that combines elements of both word and character tokenization. Byte Pair Encoding (BPE) is an example of this approach. Subword tokenization effectively captures root words found in various forms, like \"boy\" in both \"boy\" and \"boys.\" Here's how it works:\n",
    "\n",
    "Rule 1: For words that appear frequently in the dataset, do not split them into smaller subwords. These words should be retained as they are.\n",
    "\n",
    "Rule 2: For rare words that don't appear often, split them into smaller, meaningful subwords. This rule is crucial because it allows rare words to be broken down, even to the character level if necessary.\n",
    "\n",
    "This approach is a blend of word and character tokenization. The first rule keeps common words intact, similar to word tokenization. The second rule allows for further breakdown of rare words, borrowing from character tokenization.\n",
    "\n",
    "For example, if \"boy\" appears frequently in the dataset, it remains as a single token, following Rule 1. However, if \"boys\" is encountered less often, it can be split into \"boy\" and \"s,\" as \"boys\" derives from the root \"boy.\"\n",
    "\n",
    "Subword tokenization offers several advantages. Firstly, it helps models recognize that words with the same root, like \"token,\" \"tokens,\" and \"tokenizing,\" share a common meaning, which is often lost in word-based and character-based tokenization. Secondly, it aids in understanding patterns, such as the shared suffix in \"tokenization\" and \"modernization.\" The suffix \"zation\" can be treated as a subword, highlighting similar syntactic roles.\n",
    "\n",
    "Subword tokenization is significant because it allows for more nuanced word splitting. Byte Pair Encoding (BPE) is a subword tokenization algorithm used by modern language models like GPT-2 and GPT-3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4452b1-efce-4a15-8258-df1881e5ca74",
   "metadata": {},
   "source": [
    "**BPE Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deabd886-e6bb-47bb-9494-db1e59c5522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca4cfb2-ec7d-4a11-a9af-1bf24cfbed2e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "tiktoken is the python library that implements BPE\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43424e0-5d9e-4ca7-b3e2-464a552ad9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424d627-69a8-480b-bcb4-e44a93e90833",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Instantiate the byte pair tokenizer\n",
    "and encode and decode a text\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "549d1ba4-449b-470e-9926-7fabc6792f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9118d511-dcc6-4573-b99a-83ccd0019029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6af3371-c25e-4598-8836-1119b2007892",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Note the tokenId 50256 - it is the \"endoftext\" token - also shows the vocabulary size of the toeknizer - much less than 170k to 200k tokens in English Language\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bed17c59-21aa-40b6-a005-7228f4e1e983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea97a3d4-5fdb-48f9-b39f-88ac5d29da34",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Byte Pair Encoding offers several benefits, particularly in handling out-of-vocabulary words. Firstly, it helps reduce the vocabulary size. Secondly, it effectively manages unfamiliar text.\n",
    "It can handle unfamiliar text by breaking down words not found in its predefined vocabulary into smaller subword units or even individual characters. This is clearly seen in example below - for this unknown word.\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e928da7-040a-453f-9d1c-84d8d39a9a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80, 20760, 89, 81, 74]\n",
      "qytzrk\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"qytzrk\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bdffd9-990c-436e-b440-64210216d277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e9b6e2a-bf45-4aaa-9ca8-e73e89100164",
   "metadata": {},
   "source": [
    "**Data sampling with sliding window**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e41405-502f-4bd6-8f33-d04332c64dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5775\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b883861f-a75c-4b98-bb7e-423fe548e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e734cf33-d553-41cb-8f74-502e1dd42b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [11, 339, 550, 5710]\n",
      "y:      [339, 550, 5710, 465]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05b96d54-c227-4faf-bee2-c2ad04271bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11] ----> 339\n",
      "[11, 339] ----> 550\n",
      "[11, 339, 550] ----> 5710\n",
      "[11, 339, 550, 5710] ----> 465\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "104a3cc0-a105-4296-8e0c-1efb80f7e9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", ---->  he\n",
      ", he ---->  had\n",
      ", he had ---->  dropped\n",
      ", he had dropped ---->  his\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33ce1b-d133-4411-b44c-e9719b687b86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
